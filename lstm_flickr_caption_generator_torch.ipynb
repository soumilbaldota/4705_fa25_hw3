{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D_0aPXZdOoi7"
   },
   "source": [
    "# COMS W4705 - Homework 3\n",
    "## Conditioned LSTM Language Model for Image Captioning\n",
    "Daniel Bauer <bauer@cs.columbia.edu>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WHfHoPenOoi-"
   },
   "source": [
    "Follow the instructions in this notebook step-by step. Much of the code is provided (especially in part I, II, and III), but some sections are marked with **todo**. Make sure to complete all these sections. \n",
    "\n",
    "Specifically, you will build the following components: \n",
    "\n",
    "* Part I (14pts): Create encoded representations for the images in the flickr dataset using a pretrained image encoder(ResNet)\n",
    "* Part II (14pts): Prepare the input caption data.\n",
    "* Part III (24pts): Train an LSTM language model on the caption portion of the data and use it as a generator. \n",
    "* Part IV (24pts): Modify the LSTM model to also pass a copy of the input image in each timestep. \n",
    "* Part V (24pts): Implement beam search for the image caption generator.\n",
    "\n",
    "Access to a GPU is required for this assignment. If you have a recent mac, you can try using mps. Otherwise, I recommend renting a GPU instance through a service like vast.ai or lambdalabs. Google Colab can work in a pinch, but you would have to deal with quotas and it's somewhat easy to lose unsaved work. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sVv4pan6OojB"
   },
   "source": [
    "### Getting Started \n",
    "\n",
    "There are a few required packages. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sn9BUou7OojF"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import PIL # Python Image Library\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import torchvision\n",
    "from torchvision.models import ResNet18_Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available(): \n",
    "    DEVICE = 'cuda'\n",
    "elif torch.mps.is_available():\n",
    "    DEVICE = 'mps'\n",
    "else: \n",
    "    DEVICE = 'cpu'\n",
    "    print(\"You won't be able to train the RNN decoder on a CPU, unfortunately.\")\n",
    "print(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e4RdsO5hOojV"
   },
   "source": [
    "### Access to the flickr8k data\n",
    "\n",
    "We will use the flickr8k data set, described here in more detail: \n",
    "\n",
    "> M. Hodosh, P. Young and J. Hockenmaier (2013) \"Framing Image Description as a Ranking Task: Data, Models and Evaluation Metrics\", Journal of Artificial Intelligence Research, Volume 47, pages 853-899 http://www.jair.org/papers/paper3994.html \n",
    "\n",
    "N.B.: Usage of this data is limited to this homework assignment. If you would like to experiment with the dataset beyond this course, I suggest that you submit your own download request here (it's free): https://forms.illinois.edu/sec/1713398\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is available in a Google Cloud storage bucket here:\n",
    "https://storage.googleapis.com/4705_sp25_hw3/hw3data.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Download the data.\n",
    "!wget https://storage.googleapis.com/4705_sp25_hw3/hw3data.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Then unzip the data \n",
    "!unzip hw3data.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternative option if you are using Colab (though using wget, as shown above, works on Colab as well):\n",
    "* The data is available on google drive. You can access the folder here:\n",
    "https://drive.google.com/drive/folders/1sXWOLkmhpA1KFjVR0VjxGUtzAImIvU39?usp=sharing\n",
    "* Sharing is only enabled for the lionmail domain. Please make sure you are logged into Google Drive using your Columbia UNI. I will not be able to respond to individual sharing requests from your personal account. \n",
    "\n",
    "* Once you have opened the folder, click on \"Shared With Me\", then select the hw5data folder, and press shift+z. This will open the \"add to drive\" menu. Add the folder to your drive. (This will not create a copy, but just an additional entry point to the shared folder). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following variable should point to the location where the data is located. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4oTsle8kOojX"
   },
   "outputs": [],
   "source": [
    "#this is where you put the name of your data folder.\n",
    "#Please make sure it's correct because it'll be used in many places later.\n",
    "MY_DATA_DIR=\"hw3data\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AaraJrWTOojt"
   },
   "source": [
    "## Part I: Image Encodings (14 pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a77pjCYVOoju"
   },
   "source": [
    "The files Flickr_8k.trainImages.txt Flickr_8k.devImages.txt Flickr_8k.testImages.txt, contain a list of training, development, and test images, respectively. Let's load these lists. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zq_DGtSMOojx"
   },
   "outputs": [],
   "source": [
    "def load_image_list(filename):\n",
    "    with open(filename,'r') as image_list_f: \n",
    "        return [line.strip() for line in image_list_f]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FLICKR_PATH=\"hw3data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RW7otdy6Ooj5"
   },
   "outputs": [],
   "source": [
    "train_list = load_image_list(os.path.join(FLICKR_PATH, 'Flickr_8k.trainImages.txt'))\n",
    "dev_list = load_image_list(os.path.join(FLICKR_PATH,'Flickr_8k.devImages.txt'))\n",
    "test_list = load_image_list(os.path.join(FLICKR_PATH,'Flickr_8k.testImages.txt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KQB2nByoOokA"
   },
   "source": [
    "Let's see how many images there are"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DumBmc6cOokC"
   },
   "outputs": [],
   "source": [
    "len(train_list), len(dev_list), len(test_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "id": "XLYCKAmgOokI",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "Each entry is an image filename."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z35gluAGOokJ"
   },
   "outputs": [],
   "source": [
    "dev_list[20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8u0WznmKOokR"
   },
   "source": [
    "The images are located in a subdirectory.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GQSGWTzEOokS"
   },
   "outputs": [],
   "source": [
    "IMG_PATH = os.path.join(FLICKR_PATH, \"Flickr8k_Dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_H-6YFrLOokc"
   },
   "source": [
    "We can use PIL to open and display the image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1Uw2frW3Ooke"
   },
   "outputs": [],
   "source": [
    "image = PIL.Image.open(os.path.join(IMG_PATH, dev_list[20]))\n",
    "image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r3aVuqVyOokv"
   },
   "source": [
    "We are going to use an off-the-shelf pre-trained image encoder, the ResNet-18 network. Here is more detail about this model (not required for this project): \n",
    "\n",
    "> Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun; Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2016, pp. 770-778 \n",
    "> https://openaccess.thecvf.com/content_cvpr_2016/papers/He_Deep_Residual_Learning_CVPR_2016_paper.pdf\n",
    "\n",
    "The model was initially trained on an object recognition task over the ImageNet1k data. The task is to predict the correct class label for an image, from a set of 1000 possible classes.\n",
    "\n",
    "To feed the flickr images to ResNet, we need to perform the same normalization that was applied to the training images. More details here: https://pytorch.org/vision/main/models/generated/torchvision.models.resnet18.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms \n",
    "\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The resulting images, after preprocessing, are (3,224,244) tensors, where the first dimension represents the three color channels, R,G,B)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "24LAc7CQOokw"
   },
   "outputs": [],
   "source": [
    "processed_image = preprocess(image)\n",
    "processed_image.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To the ResNet18 model, the images look like this: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transforms.ToPILImage()(processed_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image Encoder\n",
    "Let's instantiate the ReseNet18 encoder. We are going to use the pretrained weights available in torchvision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_encoder = torchvision.models.resnet18(weights=ResNet18_Weights.DEFAULT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_encoder.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a prediction model,so the output is typically a softmax-activated vector representing 1000 possible object types. Because we are interested in an encoded representation of the image we are just going to use the second-to-last layer as a source of image encodings. Each image will be encoded as a vector of size 512. \n",
    "\n",
    "We will use the following hack: remove the last layer, then reinstantiate a Squential model from the remaining layers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "21ip0Uz4OolQ"
   },
   "outputs": [],
   "source": [
    "lastremoved = list(img_encoder.children())[:-1]\n",
    "img_encoder = torch.nn.Sequential(*lastremoved).to(DEVICE) # also send it to GPU memory\n",
    "img_encoder.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "id": "pMJB30MuOolb",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "Let's try the encoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image(img_name): \n",
    "    image = PIL.Image.open(os.path.join(IMG_PATH, img_name))\n",
    "    return preprocess(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_image = get_image(train_list[0])\n",
    "encoded = img_encoder(preprocessed_image.unsqueeze(0).to(DEVICE)) # unsqueeze required to add batch dim (3,224,224) becomes (1,3,224,224)\n",
    "encoded.shape                      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result isn't quite what we wanted: The final representation is actually a 1x1 \"image\" (the first dimension is the batch size). \n",
    "We can just grab this one pixel:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded = encoded[:,:,0,0] #this is our final image encoded\n",
    "encoded.shape "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lozxEAw7Ooll"
   },
   "source": [
    "**TODO:** Because we are just using the pretrained encoder, we can simply encode all the images in a preliminary step. We will store them in one big tensor (one for each dataset, train, dev, test). This will save some time when training the conditioned LSTM because we won't have to recompute the image encodings with each training epoch. We can also save the tensors to disk so that we never have to touch the bulky image data again.\n",
    "\n",
    "Complete the following function that should take a list of image names and return a tensor of size [n_images, 512] (where each row represents one image). \n",
    "\n",
    "For example `encode_imates(train_list)` should return a [6000,512] tensor. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_images(image_list): \n",
    "    pass #TODO.... \n",
    "           \n",
    "enc_images_train = encode_images(train_list)\n",
    "enc_images_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now save this to disk:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(enc_images_train, open('encoded_images_train.pt','wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TlwoEpz0Ool4"
   },
   "source": [
    "It's a good idea to save the resulting matrices, so we do not have to run the encoder again. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qp_la7ytOol6"
   },
   "source": [
    "## Part II Text (Caption) Data Preparation (14 pts)\n",
    "\n",
    "Next, we need to load the image captions and generate training data for the language model. We will train a text-only model first."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oApfN-CMOol7"
   },
   "source": [
    "### Reading image descriptions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nGsOQWQKOol7"
   },
   "source": [
    "**TODO**: Write the following function that reads the image descriptions from the file `filename` and returns a dictionary in the following format. Take a look at the file `Flickr8k.token.txt` for the format of the input file. \n",
    "The keys of the dictionary should be image filenames. Each value should be a list of 5 captions. Each caption should be a list of tokens.  \n",
    "\n",
    "The captions in the file are already tokenized, so you can just split them at white spaces. You should convert each token to lower case. You should then pad each caption with a \\<START\\> token on the left and an \\<END\\> token on the right. \n",
    "\n",
    "For example, a single caption might look like this: \n",
    "['\\<START\\>',\n",
    "  'a',\n",
    "  'child',\n",
    "  'in',\n",
    "  'a',\n",
    "  'pink',\n",
    "  'dress',\n",
    "  'is',\n",
    "  'climbing',\n",
    "  'up',\n",
    "  'a',\n",
    "  'set',\n",
    "  'of',\n",
    "  'stairs',\n",
    "  'in',\n",
    "  'an',\n",
    "  'entry',\n",
    "  'way',\n",
    "  '.',\n",
    "  '\\<EOS\\>'],"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "90KB4mEFOol8"
   },
   "outputs": [],
   "source": [
    "def read_image_descriptions(filename):    \n",
    "    image_descriptions = {}\n",
    "    \n",
    "    with open(filename,'r') as in_file:\n",
    "        pass # todo\n",
    "    \n",
    "    return image_descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.path.join(FLICKR_PATH, \"Flickr8k.token.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cy3pVpPBOol-",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "descriptions = read_image_descriptions(os.path.join(FLICKR_PATH, \"Flickr8k.token.txt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ntinjJ2mOomD"
   },
   "outputs": [],
   "source": [
    "descriptions['1000268201_693b08cb0e.jpg']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The previous line shoudl return \n",
    "<pre>[['<START>', 'a', 'child', 'in', 'a', 'pink', 'dress', 'is', 'climbing', 'up', 'a', 'set', 'of', 'stairs', 'in', 'an', 'entry', 'way', '.', '<EOS>'], ['<START>', 'a', 'girl', 'going', 'into', 'a', 'wooden', 'building', '.', '<EOS>'], ['<START>', 'a', 'little', 'girl', 'climbing', 'into', 'a', 'wooden', 'playhouse', '.', '<EOS>'], ['<START>', 'a', 'little', 'girl', 'climbing', 'the', 'stairs', 'to', 'her', 'playhouse', '.', '<EOS>'], ['<START>', 'a', 'little', 'girl', 'in', 'a', 'pink', 'dress', 'going', 'into', 'a', 'wooden', 'cabin', '.', '<EOS>']]</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XntM_NDrOomF"
   },
   "source": [
    "### Creating Word Indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9n7ogb-jOomG"
   },
   "source": [
    "Next, we need to create a lookup table from the **training** data mapping words to integer indices, so we can encode input \n",
    "and output sequences using numeric representations. \n",
    "\n",
    "**TODO** create the dictionaries id_to_word and word_to_id, which should map tokens to numeric ids and numeric ids to tokens.  \n",
    "Hint: Create a set of tokens in the training data first, then convert the set into a list and sort it. This way if you run the code multiple times, you will always get the same dictionaries. This is similar to the word indices you created for homework 3 and 4.  \n",
    "\n",
    "Make sure you create word indices for the three special tokens `<PAD>`, `<START>`, and `<EOS>` (end of sentence)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_to_word = {} #todo\n",
    "id_to_word[0] = \"<PAD>\"\n",
    "id_to_word[1] = \"<START>\"\n",
    "id_to_word[2] = \"<EOS>\"\n",
    "word_to_id = {} # todo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OalLzQ3eOomM"
   },
   "outputs": [],
   "source": [
    "word_to_id['cat'] # should print an integer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1LvZFkcgOomP"
   },
   "outputs": [],
   "source": [
    "id_to_word[1] # should print a token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MCyX0fnROomQ"
   },
   "source": [
    "Note that we do not need an UNK word token because we will only use the model as a generator, once trained."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rMF6EhC_OomS"
   },
   "source": [
    "## Part III Basic Decoder Model (24 pts)\n",
    "\n",
    "For now, we will just train a model for text generation without conditioning the generator on the image input. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aZCTlT8vOomS"
   },
   "source": [
    "We will use the LSTM implementation provided by PyTorch. The core idea here is that the recurrent layers (including LSTM) create an \"unrolled\" RNN. Each time-step is represented as a different position, but the weights for these positions are shared. We are going to use the constant MAX_LEN to refer to the maximum length of a sequence, which turns out to be 40 words in this data set (including START and END)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FrUv7QyfOomX"
   },
   "outputs": [],
   "source": [
    "MAX_LEN = max(len(description) for image_id in train_list for description in descriptions[image_id])\n",
    "MAX_LEN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iALWuacROomZ"
   },
   "source": [
    "In class, we discussed LSTM generators as transducers that map each word in the input sequence to the next word. \n",
    "<img src=\"http://www.cs.columbia.edu/~bauer/4705/lstm1.png\" width=\"480px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ntpi3gITOoma"
   },
   "source": [
    "To train the model, we will convert each description into an input output pair as follows. For example, consider the sequence \n",
    "\n",
    "`['<START>', 'a', 'black', 'dog', '<EOS>']`\n",
    "\n",
    "We would train the model using the following input/output pair (note both sequences are padded to the right up to MAX_LEN). That is, the output is simply the input shifted left (and with an extra <PAD> on the righ).\n",
    "\n",
    "output| [`a`,`back`,`dog`,`<EOS>`,`<PAD>`,`<PAD>`,...]   |\n",
    "------|--------------------------------------------------|\n",
    "input | [`<START>`,`a`,`back`,`dog`,`<EOS>`,`<PAD>`,...] |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C7iqX7aiOoma"
   },
   "source": [
    "Here is the lange model in pytorch. We will choose input embeddings of dimensionality 512 (for simplicitly, we are not initializing these with pre-trained embeddings here). We will also use 512 for the hidden state vector and the output. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn \n",
    "\n",
    "vocab_size = len(word_to_id)+1\n",
    "class GeneratorModel(nn.Module): \n",
    "    \n",
    "    def __init__(self): \n",
    "        super(GeneratorModel, self).__init__()    \n",
    "        self.embedding = nn.Embedding(vocab_size, 512) \n",
    "        self.lstm = nn.LSTM(512, 512, num_layers = 1, bidirectional=False, batch_first=True)\n",
    "        self.output = nn.Linear(512,vocab_size)\n",
    "        \n",
    "    def forward(self, input_seq): \n",
    "        hidden = self.lstm(self.embedding(input_seq))\n",
    "        out = self.output(hidden[0])\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EffglzWiOomd"
   },
   "source": [
    "The input sequence is an integer tensor of size `[batch_size, MAX_LEN]`. Each row is a vector of size MAX_LEN in which each entry is an integer representing a word (according to the `word_to_id` dictionary). If the input sequence is shorter than MAX_LEN, the remaining entries should be padded with '<PAD>'.\n",
    "\n",
    "For each input example, the model returns a distribution over possible output words. The model output is a tensor of size `[batch_size, MAX_LEN, vocab_size]`. vocab_size is the number of vocabulary words, i.e. len(word_to_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K_3J4Uf5Oome"
   },
   "source": [
    "### Creating a Dataset for the text training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0uYxdJ94Oome"
   },
   "source": [
    "**TODO**: Write a Dataset class for the text training data. The __getitem__ method should return an (input_encoding, output_encoding) pair for a single item. Both input_encoding and output_encoding should be tensors of size `[MAX_LEN]`, encoding the padded input/output sequence as illustrated above. \n",
    "\n",
    "I recommend to first read in all captions in the __init__ method and store them in a list. Above, we used the get_image_descriptions function to load the image descriptions into a dictionary. Iterate through the images in img_list, then access the corresponding captions in the `descriptions` dictionary. \n",
    "\n",
    "You can just refer back to the variables you have defined above, including `descriptions`, `train_list`, `vocab_size`, etc. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 40\n",
    "\n",
    "class CaptionDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, img_list):\n",
    "                \n",
    "        self.data = []\n",
    "\n",
    "        pass # TODO complete this method\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "        \n",
    "    def __getitem__(self,k):\n",
    "        \n",
    "        #TODO COMPLETE THIS METHOD\n",
    "\n",
    "        input_enc = None #replace\n",
    "        output_enc = None #replace\n",
    "        return input_enc, output_enc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's instantiate the caption dataset and get the first item. You want to see something like this: \n",
    "\n",
    "for the input: \n",
    "<pre>\n",
    "tensor([   1,   74,  805, 2312, 4015, 6488,  170,   74, 8686, 2312, 3922, 7922,\n",
    "        7125,   17,    2,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
    "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
    "           0,    0,    0,    0])\n",
    "</pre>\n",
    "for the output:\n",
    "<pre>\n",
    "    tensor([  74,  805, 2312, 4015, 6488,  170,   74, 8686, 2312, 3922, 7922, 7125,\n",
    "          17,    2,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
    "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
    "           0,    0,    0,    0])\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = CaptionDataset(train_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i, o = data[0]\n",
    "i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "o"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GeneratorModel().to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model(i.to(DEVICE)).shape   # should return a [40, vocab_size]  tensor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nZ47Z-NXOomi"
   },
   "source": [
    "### Training the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training function is identical to what you saw in homework 3 and 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import CrossEntropyLoss\n",
    "loss_function = CrossEntropyLoss(ignore_index = 0, reduction='mean')\n",
    "\n",
    "LEARNING_RATE = 1e-03\n",
    "optimizer = torch.optim.AdamW(params=model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "loader = DataLoader(data, batch_size = 16, shuffle = True)\n",
    "\n",
    "def train():\n",
    "    \"\"\"\n",
    "    Train the model for one epoch.\n",
    "    \"\"\"\n",
    "    tr_loss = 0\n",
    "    nb_tr_examples, nb_tr_steps = 0, 0\n",
    "    total_correct, total_predictions = 0, 0\n",
    "    tr_preds, tr_labels = [], []\n",
    "    # put model in training mode\n",
    "    model.train()\n",
    "\n",
    "    for idx, batch in enumerate(loader):\n",
    "\n",
    "        inputs,targets = batch   \n",
    "        inputs = inputs.to(DEVICE)\n",
    "        targets = targets.to(DEVICE)\n",
    "        # Run the forward pass of the model\n",
    "        logits = model(inputs)\n",
    "        loss = loss_function(logits.transpose(2,1), targets)\n",
    "        tr_loss += loss.item()\n",
    "        #print(\"Batch loss: \", loss.item()) # can comment out if too verbose.\n",
    "        nb_tr_steps += 1\n",
    "        nb_tr_examples += targets.size(0)\n",
    "\n",
    "        # Calculate accuracy\n",
    "        predictions = torch.argmax(logits, dim=2)  # Predicted token labels\n",
    "        not_pads = targets != 0  # Mask for non-PAD tokens\n",
    "        correct = torch.sum((predictions == targets) & not_pads)\n",
    "        total_correct += correct.item()\n",
    "        total_predictions += not_pads.sum().item()\n",
    "\n",
    "        if idx % 100==0:\n",
    "            #torch.cuda.empty_cache() # can help if you run into memory issues\n",
    "            curr_avg_loss = tr_loss/nb_tr_steps\n",
    "            print(f\"Current average loss: {curr_avg_loss}\")\n",
    "\n",
    "        # Run the backward pass to update parameters\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Compute accuracy for this batch\n",
    "        # matching = torch.sum(torch.argmax(logits,dim=2) == targets)\n",
    "        # predictions = torch.sum(torch.where(targets==-100,0,1))\n",
    "\n",
    "    epoch_loss = tr_loss / nb_tr_steps\n",
    "    epoch_accuracy = total_correct / total_predictions if total_predictions != 0 else 0  # Avoid division by zero\n",
    "    print(f\"Training loss epoch: {epoch_loss}\")\n",
    "    print(f\"Average accuracy epoch: {epoch_accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the training until the accuracy reaches about 0.5 (this would be high for a language model on open-domain text, but the image caption dataset is comparatively small and closed-domain). This will take about 5 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yQNWgytHOomo"
   },
   "source": [
    "### Greedy Decoder\n",
    "\n",
    "**TODO** Next, you will write a decoder. The decoder should start with the sequence `[\"<START>\", \"<PAD>\",\"<PAD>\"...]`, use the model to predict the most likely word in the next position. Append the word to the input sequence and then continue until `\"<EOS>\"` is predicted or the sequence reaches `MAX_LEN` words. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j0ztYhD8Oomp"
   },
   "outputs": [],
   "source": [
    "def decoder():\n",
    "    pass # TODO COMPLETE THIS METHHOD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jKYNQHlpOomq"
   },
   "outputs": [],
   "source": [
    "decoder()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DeigklkNOomt"
   },
   "source": [
    "this will return something like \n",
    "['a',\n",
    " 'man',\n",
    " 'in',\n",
    " 'a',\n",
    " 'white',\n",
    " 'shirt',\n",
    " 'and',\n",
    " 'a',\n",
    " 'woman',\n",
    " 'in',\n",
    " 'a',\n",
    " 'white',\n",
    " 'dress',\n",
    " 'walks',\n",
    " 'by',\n",
    " 'a',\n",
    " 'small',\n",
    " 'white',\n",
    " 'building',\n",
    " '.',\n",
    " '<EOS>']\n",
    "\n",
    "This simple decoder will of course always predict the same sequence (and it's not necessarily a good one). \n",
    "\n",
    "**TODO:** Modify the decoder as follows. Instead of choosing the most likely word in each step, sample the next word from the distribution (i.e. the softmax activated output) returned by the model. Make sure to apply torch.softmax() to convert the output activations into a distribution. \n",
    "\n",
    "To sample fromt he distribution, I recommend you take a look at [np.random.choice](https://numpy.org/doc/stable/reference/random/generated/numpy.random.choice.html), which takes the distribution as a parameter p."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RqznTk03Oomu"
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "\n",
    "def sample_decoder():\n",
    "    pass # TODO COMPLETE THIS METHOD\n",
    "\n",
    "for i in range(5):\n",
    "    print(sample_decoder())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some example outputs (it's stochastic, so your results will vary \n",
    "\n",
    "<pre>\n",
    "['<START>', 'people', 'on', 'rocky', 'ground', 'swinging', 'basketball', '<EOS>']\n",
    "['<START>', 'the', 'two', 'hikers', 'take', 'a', 'tandem', 'leap', 'while', 'another', 'is', 'involving', 'watching', '.', '<EOS>']\n",
    "['<START>', 'a', 'man', 'attached', 'to', 'a', 'bicycle', 'rides', 'a', 'motorcycle', '.', '<EOS>']\n",
    "['<START>', 'a', 'surfer', 'is', 'riding', 'a', 'wave', 'in', 'the', 'ocean', '.', '<EOS>']\n",
    "['<START>', 'a', 'child', 'plays', 'in', 'a', 'round', 'fountain', '.', '<EOS>']\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "19ERgCQ5Oomx"
   },
   "source": [
    "You should now be able to see some interesting output that looks a lot like flickr8k image captions -- only that the captions are generated randomly without any image input. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6US9JbinOomz"
   },
   "source": [
    "## Part III - Conditioning on the Image (24 pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "843GXdMSOom0"
   },
   "source": [
    "We will now extend the model to condition the next word not only on the partial sequence, but also on the encoded image. \n",
    "\n",
    "We will concatenate the 512-dimensional image representation to each 512-dimensional token embedding. The LSTM will therefore see input representations of size 1024.\n",
    "\n",
    "**TODO**: Write a new Dataset class for the combined image captioning data set. Each call to __getitem__ should return a triple  (image_encoding, input_encoding, output_encoding) for a single item. Both input_encoding and output_encoding should be tensors of size [MAX_LEN], encoding the padded input/output sequence as illustrated above. The image_encoding is the size [512] tensor we pre-computed in part I.\n",
    "\n",
    "Note: One tricky issue here is that each image corresponds to 5 captions, so you have to find the correct image for each caption. You can create a mapping from image names to row indices in the image encoding tensor. This way you will be able to find each image by it's name. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p4JyIEFGOom0"
   },
   "outputs": [],
   "source": [
    "MAX_LEN = 40\n",
    "\n",
    "class CaptionAndImage(Dataset):\n",
    "    \n",
    "    def __init__(self, img_list):\n",
    "\n",
    "        self.img_data = torch.load(open(\"encoded_images_train.pt\",'rb')) # suggested \n",
    "        self.img_name_to_id = dict([(i,j) for (j,i) in enumerate(img_list)])\n",
    "\n",
    "        self.data = []\n",
    "        # TODO COMPLETE THIS METHOD\n",
    "                \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "        \n",
    "    def __getitem__(self,k):\n",
    "        # TODO COMPLETE THIS METHOD\n",
    "        img_data = None #replace\n",
    "        input_enc = None #replace\n",
    "        output_enc = None #replace\n",
    "        \n",
    "        return img_data, input_enc, output_enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joint_data = CaptionAndImage(train_list)\n",
    "img, i, o = data[0]\n",
    "img.shape # should return torch.Size([512])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i.shape # should return torch.Size([40])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "o.shape # should return torch.Size([40])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO: Updating the model**\n",
    "Update the language model code above to include a copy of the image for each position. \n",
    "The forward function of the new model should take two inputs: \n",
    "    \n",
    "   1. a `(batch_size, 2048)` ndarray of image encodings. \n",
    "   2. a `(batch_size, MAX_LEN)` ndarray of partial input sequences. \n",
    "    \n",
    "And one output as before: a `(batch_size, vocab_size)` ndarray of predicted word distributions.   \n",
    "\n",
    "The LSTM will take input dimension 1024 instead of 512 (because we are concatenating the 512-dim image encoding). \n",
    "\n",
    "In the forward function, take the image and the embedded input sequence (i.e. AFTER the embedding was applied), and concatenate the image to each input. This requires some tensor manipulation. I recommend taking a look at [torch.Tensor.expand](https://pytorch.org/docs/stable/generated/torch.Tensor.expand.html) and [torch.Tensor.cat](https://pytorch.org/docs/stable/generated/torch.Tensor.cat.html).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(word_to_id)+1\n",
    "\n",
    "class CaptionGeneratorModel(nn.Module): \n",
    "    \n",
    "    def __init__(self): \n",
    "        super(CaptionGeneratorModel, self).__init__()    \n",
    "        # TODO COMPLETE THIS METHOD\n",
    "        \n",
    "    def forward(self, img, input_seq): \n",
    "\n",
    "        # TODO COMPLETE THIS METHOD\n",
    "        out = None # replace\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wcifvfqiOom3"
   },
   "source": [
    "Let's try this new model on one item: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CaptionGeneratorModel().to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item = joint_data[0]\n",
    "img, input_seq, output_seq = item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = model(img.unsqueeze(0).to(DEVICE), input_seq.unsqueeze(0).to(DEVICE))\n",
    "\n",
    "logits.shape # should return (1,40,8922) = (batch_size, MAX_LEN, vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training function is, again, mostly unchanged. Keep training until the accuracy exceeds 0.5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import CrossEntropyLoss\n",
    "loss_function = CrossEntropyLoss(ignore_index = 0, reduction='mean')\n",
    "\n",
    "LEARNING_RATE = 1e-03\n",
    "optimizer = torch.optim.AdamW(params=model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "loader = DataLoader(data, batch_size = 16, shuffle = True)\n",
    "\n",
    "def train():\n",
    "    \"\"\"\n",
    "    Train the model for one epoch.\n",
    "    \"\"\"\n",
    "    tr_loss = 0\n",
    "    nb_tr_examples, nb_tr_steps = 0, 0\n",
    "    total_correct, total_predictions = 0, 0\n",
    "    tr_preds, tr_labels = [], []\n",
    "    # put model in training mode\n",
    "    model.train()\n",
    "\n",
    "    for idx, batch in enumerate(loader):\n",
    "        \n",
    "        img, inputs,targets = batch  \n",
    "        img = img.to(DEVICE)\n",
    "        inputs = inputs.to(DEVICE)\n",
    "        targets = targets.to(DEVICE)\n",
    "        \n",
    "        # Run the forward pass of the model\n",
    "        logits = model(img, inputs)\n",
    "        loss = loss_function(logits.transpose(2,1), targets)\n",
    "        tr_loss += loss.item()\n",
    "        #print(\"Batch loss: \", loss.item()) # can comment out if too verbose.\n",
    "        nb_tr_steps += 1\n",
    "        nb_tr_examples += targets.size(0)\n",
    "\n",
    "        # Calculate accuracy\n",
    "        predictions = torch.argmax(logits, dim=2)  # Predicted token labels\n",
    "        not_pads = targets != 0  # Mask for non-PAD tokens\n",
    "        correct = torch.sum((predictions == targets) & not_pads)\n",
    "        total_correct += correct.item()\n",
    "        total_predictions += not_pads.sum().item()\n",
    "\n",
    "        if idx % 100==0:\n",
    "            #torch.cuda.empty_cache() # can help if you run into memory issues\n",
    "            curr_avg_loss = tr_loss/nb_tr_steps\n",
    "            print(f\"Current average loss: {curr_avg_loss}\")\n",
    "\n",
    "        # Run the backward pass to update parameters\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Compute accuracy for this batch\n",
    "        # matching = torch.sum(torch.argmax(logits,dim=2) == targets)\n",
    "        # predictions = torch.sum(torch.where(targets==-100,0,1))\n",
    "\n",
    "    epoch_loss = tr_loss / nb_tr_steps\n",
    "    epoch_accuracy = total_correct / total_predictions if total_predictions != 0 else 0  # Avoid division by zero\n",
    "    print(f\"Training loss epoch: {epoch_loss}\")\n",
    "    print(f\"Average accuracy epoch: {epoch_accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO: Testing the model**: \n",
    "Rewrite the greedy decoder from above to take an encoded image representation as input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_decoder(img):\n",
    "    #TODO: Complete this method\n",
    "    \n",
    "    result = None  # replace\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can load one of the dev images, pass it through the preprocessor and the image encoder, and then into the decoder!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_img = PIL.Image.open(os.path.join(IMG_PATH, dev_list[199]))\n",
    "preprocessed_img = preprocess(raw_img).to(DEVICE)\n",
    "encoded_img = img_encoder(preprocessed_img.unsqueeze(0)).reshape((512))\n",
    "caption = sample_decoder(encoded_img)\n",
    "print(caption)\n",
    "raw_img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lLqklyJOOom4"
   },
   "source": [
    "The result should look pretty good for most images, but the model is prone to hallucinations. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hifHwKUtOonO"
   },
   "source": [
    "## Part IV - Beam Search Decoder (24 pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OrniTyvEOonP"
   },
   "source": [
    "**TODO** Modify the simple greedy decoder for the caption generator to use beam search. \n",
    "Instead of always selecting the most probable word, use a *beam*, which contains the n highest-scoring sequences so far and their total probability (i.e. the product of all word probabilities). I recommend that you use a list of `(probability, sequence)` tuples. After each time-step, prune the list to include only the n most probable sequences. \n",
    "\n",
    "Then, for each sequence, compute the n most likely successor words. Append the word to produce n new sequences and compute their score. This way, you create a new list of n*n candidates. \n",
    "\n",
    "Prune this list to the best n as before and continue until `MAX_LEN` words have been generated. \n",
    "\n",
    "Note that you cannot use the occurence of the `\"<EOS>\"` tag to terminate generation, because the tag may occur in different positions for different entries in the beam. \n",
    "\n",
    "Once `MAX_LEN` has been reached, return the most likely sequence out of the current n. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g4gQ4H-1OonP"
   },
   "outputs": [],
   "source": [
    "def img_beam_decoder(n, img):\n",
    "   \n",
    "    # TODO: Complete this method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ro_iyz1bOonT"
   },
   "source": [
    "**TODO** Finally, before you submit this assignment, please show 3 development images, each with 1) their greedy output, 2) beam search at n=3 3) beam search at n=5. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "K_3J4Uf5Oome",
    "nZ47Z-NXOomi",
    "yQNWgytHOomo",
    "6US9JbinOomz",
    "hifHwKUtOonO"
   ],
   "name": "flickr_caption_generator.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
